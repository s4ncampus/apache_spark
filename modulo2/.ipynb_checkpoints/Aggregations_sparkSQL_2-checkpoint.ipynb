{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregations\n",
    "Su objetivo es resumir y agrupar información númerica de grandes conjuntos de información, para ello es necesario definir: una llave o grouping y una función de Agregación. Existen muchas funciones de agregación en Apache Spark, que son de vital importancia cuando se requiere hacer analisis de los datos. Entre los grupos tenemos Simples, Grouping, Window, GroupingSet, Rollup y Cube. En este notebook vamos a ir describiendo cada uno de los grupos con ejemplos que podran ejecutar.\n",
    "\n",
    "## Grouping\n",
    "En muchas ocaciones necesitamos agrupar la información para efectuar algunas operaciones dentro del grupo\n",
    "\n",
    "Veamos algunos ejemplos:\n",
    "Vamos a tomar el mismo conjunto de datos https://www.kaggle.com/sidtwr/videogames-sales-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//Import para Jupyter-notebooks \n",
    "import $ivy.`org.apache.spark::spark-sql:2.4.0`\n",
    "import $ivy.`sh.almond::almond-spark:0.6.0`\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)\n",
    "\n",
    "import org.apache.spark.sql._\n",
    "\n",
    "val spark = {\n",
    "  NotebookSparkSession.builder()\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    "}\n",
    "\n",
    "val df = spark.read.format(\"csv\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .load(\"resources/vgsales.csv\")\n",
    "df.createOrReplaceTempView(\"vgsales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spark.implicits._\n",
    "import org.apache.spark.sql.functions._\n",
    "df.groupBy(\"Platform\", \"Year\").count().sort(desc(\"count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select Platform, Year, count(1) as count from vgsales group by Platform, Year order by count Desc\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tambien podemos utilizar las funciones vistas anteriormente.\n",
    "`Calcular la sumatoria de ventas totales agrupados por genero`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy(\"Genre\").agg(\n",
    "  sum(\"Global_sales\").alias(\"globalSales\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tambien podemos utilizar `expr` para obtener el mismo resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy(\"Genre\").agg(\n",
    "  expr(\"sum(Global_sales)\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark tambien ofrece trabajar agregaciones dentro de mapas, para que la especificación de las mimas sea más facil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy(\"Genre\").agg(\"Global_sales\"->\"avg\", \"Global_sales\"->\"sum\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Window functions\n",
    "Estas funciones son similiares a las funciones anteriores de Group-by sin embargo, en un group-by todas las filas  van en un sólo grupo mientras que en las Window functions se obtiene un valor por cada fila de una tabla basado en un grupo de filas llamdos *frames*.\n",
    "<img src=\"resources/windows.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('Name,'Platform,'Year,'Publisher,'Global_Sales).where($\"Platform\" === \"PC\" && $\"Year\" <= 1994).orderBy($\"Year\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.expressions.Window\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val windowSpec = Window\n",
    "  .partitionBy(\"Platform\") // Cómo dividir el grupo\n",
    "  .orderBy(col(\"Year\")) // Orden dentro de la partición\n",
    "  .rowsBetween(Window.unboundedPreceding, Window.currentRow) //Especificación del frame\n",
    "\n",
    "df.where(\"Year IS NOT NULL\")\n",
    "  .select(\n",
    "    col(\"Platform\"),\n",
    "    col(\"Year\"),\n",
    "    col(\"Global_Sales\"),\n",
    "    sum(\"Global_Sales\").over(windowSpec).alias(\"Global_Sales ACC\")\n",
    "  ).distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este mismo resultado se puede obtener si corremos una sentencia SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "Platform,\n",
    "Year,\n",
    "Global_Sales, \n",
    "sum(Global_Sales) over ( partition by Platform order by Year ROWS BETWEEN\n",
    "                        UNBOUNDED PRECEDING AND\n",
    "                        CURRENT ROW ) as GlobalSalesAcc\n",
    "FROM vgsales\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping sets\n",
    "En las anteriores funciones se realizaban agregaciones sobre un cojunto de columnas con sus respectivos valores, Cuando se quiere algo más complejo cómo una agregación sobre multiples grupos es posible utilizar grouping sets. **Esta función sólo esta disponible en SQL, para utilizarla con Dataframes se podra utilizar `rollup` y `cube`**\n",
    "\n",
    "En el siguiente caso queremos tener agrupaciones del Año con la plataforma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT Year, Platform, count(Platform) FROM vgsales\n",
    "GROUP BY Year, Platform GROUPING SETS((Year, Platform),())\n",
    "ORDER BY Year DESC, Platform \n",
    "\"\"\").show(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rollup \n",
    "Es una agregación multidimensional que ejecuta una variedad de group-by dentro del conjunto de datos, entre los resultados se puede ver las columnas que hacen parte del rollup en `null` para especificar el agrupamiento realizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " df.where(\"Year > 2012\").groupBy(\"Year\",\"Platform\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "val rolledUpDF = df.where(\"Year > 2012\").rollup(\"Year\", \"Platform\").agg(count(\"Platform\"))\n",
    "  .selectExpr(\"Year\", \"Platform\", \"`count(Platform)` as total_quantity\")\n",
    "  .sort(desc(\"Year\"), col(\"Platform\"))\n",
    "rolledUpDF.show(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolledUpDF.where(\"Year is NULL\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolledUpDF.where(\"Platform is NULL\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cube\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.where(\"Year > 2012\").cube(\"Year\", \"Platform\").agg(count(col(\"Platform\")))\n",
    "  .select(\"Year\", \"Platform\", \"count(Platform)\").orderBy(\"Year\").show(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
