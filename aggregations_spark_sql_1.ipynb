{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregations\n",
    "Su objetivo es resumir y agrupar información númerica de grandes conjuntos de información, para ello es necesario definir: una llave o grouping y una función de Agregación.\n",
    "Existen muchas funciones de agregación en Apache Spark, que son de vital importancia cuando se requiere hacer analisis de los datos. \n",
    "Entre los grupos tenemos Simples, Grouping,  Window, GroupingSet, Rollup y Cube.\n",
    "En este notebook vamos a ir describiendo cada uno de los grupos con ejemplos que podran ejecutar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple\n",
    "\n",
    "Son las funciones de Agregación mas sencillas, entregan información resumida de un conjunto de información completo. \n",
    "Todas las funciones se pueden encontrar aca: https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$\n",
    "A continuación se muestran algunas de las más populares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### count\n",
    "En este caso vamos a ver que este count no va a ser una action si no una transformación, podemos definir si queremos hacer como en SQL  un count(*), count(1) o un count(\"columna\"). \n",
    "\n",
    "Vamos a tomar un conjunto de datos https://www.kaggle.com/sidtwr/videogames-sales-dataset para trabajar con estas funciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//Import para Jupyter-notebooks \n",
    "import $ivy.`org.apache.spark::spark-sql:2.4.0`\n",
    "import $ivy.`sh.almond::almond-spark:0.6.0`\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)\n",
    "\n",
    "import org.apache.spark.sql._\n",
    "\n",
    "val spark = {\n",
    "  NotebookSparkSession.builder()\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df = spark.read.format(\"csv\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .load(\"resources/vgsales.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"vgsales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.count\n",
    "df.select(count(\"*\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select count(*) from vgsales\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountDistinct\n",
    "\n",
    "En ocaciones queremos contar unicamente los registros que son únicos en una columna especifica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.countDistinct\n",
    "time(df.select(countDistinct(\"Genre\")).show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select count(distinct Genre) from vgsales\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### approx_count_distinct\n",
    "Cuando se trabaja con grandes volumenes de información, tener un número que se aproxime a la cantidad real es valioso, con esta función podemos definir el máximo error permitido. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.approx_count_distinct\n",
    "time(df.select(approx_count_distinct(\"Genre\", 0.1).as(\"approxCount\")).show() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select APPROX_COUNT_DISTINCT(Genre) as approxCount from vgsales\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El tiempo que se tomo el count exacto fue un poco mas de 3segundos y este ulitmo solo 0.2 segundos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first and last\n",
    "Esta función permite retornar el primer y ultimo registro de nuestro dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{first, last}\n",
    "df.select(first(\"Name\"), last(\"Name\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select first(name), last(name) from vgsales\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### min and max\n",
    "Obtiene el valor mínimo y máximo de la columna deseada. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{min, max,col}\n",
    "df.where(col(\"Publisher\") === \"Nintendo\").select(min(\"Global_Sales\"), max(\"Global_Sales\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT min(Global_Sales), max(Global_Sales) \n",
    "FROM vgsales WHERE Publisher = 'Nintendo'\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sum\n",
    "Realiza la sumatoria de todos los valores en una sola fila"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.sum\n",
    "df.select(sum(\"Global_Sales\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select sum(Global_Sales) from vgsales\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sumDistinct\n",
    "Adicional a la suma de todos los registros, puedes sumar únicamente los valores distintos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.sumDistinct\n",
    "df.select(sumDistinct(\"Global_Sales\")).show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average (avg o mean)\n",
    "En otro escenario tendriamos que primero sumar todos lo registros y dividirlos por el *count*, Spark provee una manera mas facirl de obtener el promedio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{sum, count, avg, expr}\n",
    "\n",
    "df.select(\n",
    "    count(\"*\").alias(\"# video games\"),\n",
    "    sum(\"NA_Sales\").alias(\"North America Sales\"),\n",
    "    avg(\"NA_Sales\").alias(\"avg NA Sales\"),\n",
    "    expr(\"mean(NA_Sales)\").alias(\"mean NA Sales\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT count(*) as num_video_games, sum(NA_Sales) as NorthAmericaSales, avg(NA_Sales) \n",
    "FROM vgsales\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Varianza y desviación estándar \n",
    "Spark ofrece estas y algunas otras funciones mas especificas para analisis estadisticos*. Para estas se pueden calcular muestrales o poblacionales.\n",
    "* Asimetria y curtosis  `org.apache.spark.sql.functions.{skewness, kurtosis}`\n",
    "* Covarianza y correlación `import org.apache.spark.sql.functions.{corr, covar_pop, covar_samp}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{var_pop, stddev_pop}\n",
    "import org.apache.spark.sql.functions.{var_samp, stddev_samp}\n",
    "df.select(var_pop(\"Global_Sales\"), var_samp(\"Global_Sales\"),\n",
    "  stddev_pop(\"Global_Sales\"), stddev_samp(\"Global_Sales\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, en Spark se puede obtener información general de cada columna con describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(col(\"Platform\"), col(\"Global_Sales\")).describe().show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
